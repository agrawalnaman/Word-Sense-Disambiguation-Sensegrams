{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "### Sense Determination\n",
    "\n",
    "We calculate the sense of a target word in a particular context by maximizing the cosine similarity between the aggregate context vector (average of the context word vectors after removing the stop words) and the different sense vectors of the target word.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "For evaluation, we check the key to **group** all the sentences in the test data which have the same sense for the same target word. Then we run the function on all these sentences (of a *group*) to check whether most of them (ideally all of them) have the same index or not.\n",
    "\n",
    "On running the function on all these sentences (of a *group*) we get the sense indices. We are making an assumption here, that is, the most common sense index that we are obtaining is the correct sense index for this *group* of sentences. Then the measure of accuracy is calculated using the formula:\n",
    "\n",
    "```example\n",
    "accuracy = ∑(g) #(most_common_index(g)) / total_sentences\n",
    "```\n",
    "\n",
    "where `#(most_common_index(g))` gives the number of occurences of the most common index on running the function on a *group* `g` and `total_sentences` is the total number of sentences in the test dataset which give a valid output on running the function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation using a Sensegram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initializations\n",
    "\n",
    "We need to import `numpy` for working with arrays, and other libs like `os`, `pickle` and `pprint` for other utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f97618ca853e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'"
     ]
    }
   ],
   "source": [
    "import os, pprint, pickle, re\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "#TEST_SENTENCES_PATH = '/Users/sounak/Documents/clg/nlp/nlp-projects/data/wsd/sentences.txt'\n",
    "TEST_SENTENCES_PATH = '/home/nmn/Downloads/nlp-projects-master/data/wsd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "The two helper functions `save_obj` and `load_obj` are used to pickle any object and load back the pickle file. These functions will be useful in saving the vector dicts and thus faster loading of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Sensegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2bb59ccf40ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#SENSEGRAM_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/sensegrams_of_wikipedia_cluster\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mSENSEGRAM_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENSEGRAM_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msense_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP'"
     ]
    }
   ],
   "source": [
    "sense_vecs = load_obj('sense_vecs')\n",
    "pos_tags = load_obj('pos_tags')\n",
    "\n",
    "if not (sense_vecs and pos_tags):\n",
    "    #SENSEGRAM_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/sensegrams_of_wikipedia_cluster\"\n",
    "    SENSEGRAM_PATH = \"/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP\"\n",
    "    f = open(SENSEGRAM_PATH, 'r')\n",
    "    sense_vecs = {}\n",
    "    pos_tags = set()\n",
    "\n",
    "    for line in f.readlines():\n",
    "        t = line.split('\\t')\n",
    "        word, pos = t[0].split('#')\n",
    "        pos_tags.add(pos)\n",
    "        if t[1] == '0':\n",
    "            sense_vecs[(word, pos)] = []\n",
    "        sense_vecs[(word, pos)].append(np.array(eval(t[2])))\n",
    "    f.close()\n",
    "    save_obj(sense_vecs, 'sense_vecs')\n",
    "    save_obj(pos_tags, 'pos_tags')\n",
    "\n",
    "print('sense_vecs have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d7e882de90ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#GLOVE_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/glove.6B.300d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mGLOVE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP'"
     ]
    }
   ],
   "source": [
    "word_vecs = load_obj('word_vecs')\n",
    "\n",
    "if not word_vecs:\n",
    "    #GLOVE_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/glove.6B.300d.txt\"\n",
    "    GLOVE_PATH = \"/home/nmn/Desktop/JOBIMTEXT_NAMAN_PRIYANKA_INTERNSHIP\"\n",
    "    f = open(GLOVE_PATH, 'r')\n",
    "    word_vecs = {}\n",
    "    for line in f.readlines():\n",
    "        t = line.split(' ')\n",
    "        word_vecs[t[0]] = np.array([float(_) for _ in t[1:]])\n",
    "    f.close()\n",
    "    save_obj(word_vecs, 'word_vecs')\n",
    "    \n",
    "print('word_vecs have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Sense\n",
    "\n",
    "The function `compute_sense_idx` takes a sentence, the target and some other arguments and returns the index of the sense of the target that was used in the current context.\n",
    "\n",
    "This function maximizes the cosine similarity of an aggregate context vector with the vectors of the different senses of the target word. It also doesn't include the stop words in the context. The aggregate context vector is calculated using the lemmatized words in the context after removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4cf2813a674a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_sense_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = get_stop_words('en')\n",
    "\n",
    "def compute_sense_idx(sentence, target):\n",
    "    if target not in sentence:\n",
    "        return None\n",
    "    sentence = nltk.pos_tag(sentence)\n",
    "    context = list(filter(lambda x: x[0] != target, sentence))\n",
    "    sum = np.zeros(300)\n",
    "    preprocess = lambda w, pos : (lem.lemmatize(w, pos[0].lower()), pos) if pos[0].lower() in ['a', 'r', 'n', 'v'] else (w, pos)\n",
    "    context_final = [preprocess(w, pos) for w, pos in context if w not in stop_words]\n",
    "    for w, _ in context_final:\n",
    "        try:\n",
    "            sum += word_vecs[w]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "    cw_mean = np.divide(sum, len(context))\n",
    "    max_idx = -1\n",
    "    max_value = float('-inf')\n",
    "    for pos in pos_tags:\n",
    "        try:\n",
    "            for idx, sense in enumerate(sense_vecs[(target, pos)]):\n",
    "                if np.linalg.norm(sense) > 0:\n",
    "                    result = np.divide(np.dot(sense, cw_mean), (np.linalg.norm(sense) * np.linalg.norm(cw_mean)))\n",
    "                    if result > max_value:\n",
    "                        max_value = result\n",
    "                        max_idx = idx\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "This is a light-weight tokenizer for tokenizing the input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = [_.lower() for _ in re.split(r\"[^a-zA-ZÀ-ÿ0-9']+\", text)]\n",
    "    words = [_[:-2] if \"'s\" in _ else _ for _ in words]\n",
    "    return list(filter(('').__ne__, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We are testing the function on the SemEval Test Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_sense_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-02504767d0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_sense_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results have been loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_sense_idx' is not defined"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "\n",
    "for k, v in sents.items():\n",
    "    res['.'.join(k.split('.')[:-1])] = compute_sense_idx(v, k.split('.')[-1])\n",
    "    \n",
    "print('results have been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sentences have been loaded\n"
     ]
    }
   ],
   "source": [
    "from xml.dom.minidom import parse\n",
    "FILE = './data/wsd-test/contexts/senseval2-format/semeval-2013-task-13-test-data.senseval2.xml'\n",
    "\n",
    "dom = parse(FILE)\n",
    "inst = dom.getElementsByTagName('instance')\n",
    "\n",
    "sents = {}\n",
    "\n",
    "for i in inst:\n",
    "    k = i.attributes['id'].value\n",
    "    context = i.getElementsByTagName('context')[0]\n",
    "    word = context.getElementsByTagName('head')[0].childNodes[0].nodeValue\n",
    "    v = ' {} '.format(word).join(t.nodeValue.strip() for t in context.childNodes if t.nodeType == t.TEXT_NODE)\n",
    "    sents[k + '.' + word] = tokenize(v)\n",
    "\n",
    "print('test sentences have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys have been loaded\n"
     ]
    }
   ],
   "source": [
    "KEY = './data/wsd-test/keys/gold/all.singlesense.key'\n",
    "\n",
    "keys = {}\n",
    "keys_rev = {}\n",
    "f = open(KEY, 'r')\n",
    "for line in f.readlines():\n",
    "    l = line.strip().split(' ')\n",
    "    keys[l[1]] = l[2].split(':')[0].split('%')[1]\n",
    "    try:\n",
    "        keys_rev[l[0] + '%' + l[2].split(':')[0].split('%')[1]].append(l[1])\n",
    "    except KeyError:\n",
    "        keys_rev[l[0] + '%' + l[2].split(':')[0].split('%')[1]] = [l[1]]\n",
    "    \n",
    "print('keys have been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'add.v.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe80ac65a2ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys_rev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys_rev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fe80ac65a2ad>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys_rev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys_rev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'add.v.1'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total = correct = 0\n",
    "\n",
    "for k in keys_rev.keys():\n",
    "    c = Counter([res[_] for _ in keys_rev[k]])\n",
    "    del c[-1]\n",
    "    del c[None]\n",
    "    correct += c.most_common(1)[0][1]\n",
    "    total += sum(c.values())\n",
    "    \n",
    "print(correct / total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
